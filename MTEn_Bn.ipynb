{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of MachineTranslationEn-Bn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasiburrahman1/NLP/blob/master/Copy_of_MachineTranslationEn_Bn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnhpcQJY5XC",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation English to Bangla with RNN\n",
        "\n",
        "Machine Translation using Recurrent Neural Network\n",
        "> License https://www.apache.org/licenses/LICENSE-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKM0J7SY5XE",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "- Install Anaconda `Jupyter` Notebook.\n",
        "- Install Python3.\n",
        "- With Anaconda Prompt install `Tensorflow Keras` NLP Tools. For Faster Processing need GPU processing. \n",
        "    - Keras\n",
        "```batch\n",
        "   conda install -c conda-forge tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2zIvMc6yhm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ef5d86d-4ecc-4654-baff-a6195fc2fc21"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWVprBmpY5XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkKjrUOHY5XJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f7a25878-ebc8-4a00-ee83-237a199c10f3"
      },
      "source": [
        "# check up GPU/CPU processing power\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13678921496256568944\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 12758523906297544191\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcdT2VfOY5XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset path\n",
        "path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/datasetCleaned.txt\"\n",
        "#path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/test_Text_Document.txt\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrJEUQPY5XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "#     remove punctuations from lines\n",
        "    w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d3rfmJcY5XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabularyCounter(dataFileEn, dataFileBn):\n",
        "    # setup Vocabulary \n",
        "    english_words = collections.Counter([word for sentence in dataFileEn for word in sentence.split()])\n",
        "    bangla_words = collections.Counter([word for sentence in dataFileBn for word in sentence.split()])\n",
        "    \n",
        "    print('Total English words: {}'.format(len([word for sentence in dataFileEn for word in sentence.split()])))\n",
        "    print('Unique English words: {}'.format(len(english_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*english_words.most_common(10)))[0]) + '\"\\n')\n",
        "\n",
        "    print('Total Bangla words: {}'.format(len([word for sentence in dataFileBn for word in sentence.split()])))\n",
        "    print('Unique Bangla words: {}'.format(len(bangla_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*bangla_words.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpulyhIUY5XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, BANGLA]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    allEnLines = []\n",
        "    allBnLines = []\n",
        "    for l in lines[:num_examples]:\n",
        "        w = l.split('\\t')\n",
        "        allEnLines.append(w[0])\n",
        "        allBnLines.append(w[1])\n",
        "    vocabularyCounter(allEnLines, allBnLines) \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "#     rearrange dataset src and target\n",
        "    for indd in range(len(word_pairs)):\n",
        "        temp = word_pairs[indd][0]\n",
        "        word_pairs[indd][0] = word_pairs[indd][1]\n",
        "        word_pairs[indd][1] = temp\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T4YavuYY5Xa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "29e83d9c-55de-497d-dfd8-05f9741e2063"
      },
      "source": [
        "# preprocessing function declear and vocabulary checking\n",
        "bn, en = create_dataset(path_to_file, None)\n",
        "print(\"\\n\\n\", en[4])\n",
        "print(bn[4])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 19606\n",
            "Unique English words: 2839\n",
            "Most Common Words: \"I\" \"Tom\" \"to\" \"you\" \"is\" \"the\" \"a\" \"I'm\" \"your\" \"He\"\n",
            "\n",
            "Total Bangla words: 19000\n",
            "Unique Bangla words: 3527\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"কি\" \"না।\" \"আপনি\" \"এটা\" \"তুমি\" \"কথা\" \"আপনার\"\n",
            "\n",
            "\n",
            " <start> Run ! <end>\n",
            "<start> পালান ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-MgBVD5Y5Xe",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sew4h85PY5Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function declearation for \n",
        "#     1. length of sentence\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#     2. tokenizer\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "#     3. load dataset\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang  = create_dataset(path, num_examples)\n",
        "    \n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ki0zuXY5Xh",
        "colab_type": "text"
      },
      "source": [
        "## Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvjgS2pyY5Xh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4e56b98f-c31e-4e28-d138-4c581e5e3936"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(\"Size:- Train_X-\", len(input_tensor_train), \"Train_Y-\", len(target_tensor_train), \"Test_X-\", len(input_tensor_val), \"Test_Y-\", len(target_tensor_val))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 2425\n",
            "Unique English words: 555\n",
            "Most Common Words: \"I\" \"Tom\" \"Tom.\" \"I'm\" \"is\" \"He\" \"Who\" \"You\" \"me.\" \"it.\"\n",
            "\n",
            "Total Bangla words: 2645\n",
            "Unique Bangla words: 770\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"টমকে\" \"আমরা\" \"আমাকে\" \"না।\" \"এটা\" \"কি\" \"করুন।\"\n",
            "Size:- Train_X- 800 Train_Y- 800 Test_X- 200 Test_Y- 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjgMFIzaY5Xj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e2d7a57d-7cf4-41d5-f469-af8fd4a4aa87"
      },
      "source": [
        "# showing example of tokenization\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "9 ----> is\n",
            "46 ----> that\n",
            "336 ----> all\n",
            "6 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "535 ----> অার\n",
            "71 ----> কিছু\n",
            "5 ----> ?\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M04kXdwjY5Xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "655916ff-4091-469f-f3c4-09a60fab06b6"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print(\"Input-output shape batch\", example_input_batch.shape, example_target_batch.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input-output shape batch (64, 7) (64, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ChByDVVY5Xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e6b50940-449f-45d2-f0d5-67ca6f221c0a"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, activation='linear',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 7, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb0TGNahY5Xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e84c588-3163-4665-e76e-ed8dfd9d5cb9"
      },
      "source": [
        "# Attention and context_vector\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.sigmoid(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    attention_weights = tf.nn.sigmoid(score)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh3amf2uY5Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8962bd7e-c2d6-48e6-b95e-92860dbc70e2"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, activation='tanh',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 673)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4pWS6iMY5Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk7BlC05Y5Xv",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoint saving object "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH44B_ifY5Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoint for saving the object\n",
        "checkpoint_dir = './training_checkpoints_saved'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpointSaved\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnx9VvCAY5Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIYhouznY5X0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ebf1073-a68e-4110-dd9e-4318b70a3f32"
      },
      "source": [
        "# start learn processing\n",
        "EPOCHS = 100\n",
        "lossMatrix = []\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    lossMatrix.append(total_loss / steps_per_epoch)\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.3679\n",
            "Epoch 2 Batch 0 Loss 2.0454\n",
            "Epoch 2 Loss 2.0905\n",
            "Time taken for 1 epoch 21.737708806991577 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8552\n",
            "Epoch 4 Batch 0 Loss 1.7138\n",
            "Epoch 4 Loss 1.6999\n",
            "Time taken for 1 epoch 21.647706747055054 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.6100\n",
            "Epoch 6 Batch 0 Loss 1.4034\n",
            "Epoch 6 Loss 1.2956\n",
            "Time taken for 1 epoch 21.955848932266235 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1340\n",
            "Epoch 8 Batch 0 Loss 0.9851\n",
            "Epoch 8 Loss 0.9764\n",
            "Time taken for 1 epoch 21.838343143463135 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8657\n",
            "Epoch 10 Batch 0 Loss 0.6960\n",
            "Epoch 10 Loss 0.7053\n",
            "Time taken for 1 epoch 21.99728536605835 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5172\n",
            "Epoch 12 Batch 0 Loss 0.4177\n",
            "Epoch 12 Loss 0.4983\n",
            "Time taken for 1 epoch 23.335477590560913 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4106\n",
            "Epoch 14 Batch 0 Loss 0.3346\n",
            "Epoch 14 Loss 0.3415\n",
            "Time taken for 1 epoch 22.618211269378662 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2595\n",
            "Epoch 16 Batch 0 Loss 0.2244\n",
            "Epoch 16 Loss 0.2478\n",
            "Time taken for 1 epoch 21.91493272781372 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2027\n",
            "Epoch 18 Batch 0 Loss 0.1851\n",
            "Epoch 18 Loss 0.2020\n",
            "Time taken for 1 epoch 21.740707874298096 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1719\n",
            "Epoch 20 Batch 0 Loss 0.1212\n",
            "Epoch 20 Loss 0.1674\n",
            "Time taken for 1 epoch 21.35605549812317 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1449\n",
            "Epoch 22 Batch 0 Loss 0.1151\n",
            "Epoch 22 Loss 0.1540\n",
            "Time taken for 1 epoch 21.523517370224 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1309\n",
            "Epoch 24 Batch 0 Loss 0.0975\n",
            "Epoch 24 Loss 0.1413\n",
            "Time taken for 1 epoch 21.623517513275146 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1061\n",
            "Epoch 26 Batch 0 Loss 0.1108\n",
            "Epoch 26 Loss 0.1356\n",
            "Time taken for 1 epoch 21.869019746780396 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1129\n",
            "Epoch 28 Batch 0 Loss 0.1025\n",
            "Epoch 28 Loss 0.1278\n",
            "Time taken for 1 epoch 21.506020069122314 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0894\n",
            "Epoch 30 Batch 0 Loss 0.1015\n",
            "Epoch 30 Loss 0.1229\n",
            "Time taken for 1 epoch 21.79686975479126 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.1014\n",
            "Epoch 32 Batch 0 Loss 0.1022\n",
            "Epoch 32 Loss 0.1203\n",
            "Time taken for 1 epoch 21.582808256149292 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0991\n",
            "Epoch 34 Batch 0 Loss 0.1053\n",
            "Epoch 34 Loss 0.1193\n",
            "Time taken for 1 epoch 21.672070026397705 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0963\n",
            "Epoch 36 Batch 0 Loss 0.0892\n",
            "Epoch 36 Loss 0.1136\n",
            "Time taken for 1 epoch 21.911744832992554 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.1000\n",
            "Epoch 38 Batch 0 Loss 0.1179\n",
            "Epoch 38 Loss 0.1117\n",
            "Time taken for 1 epoch 21.68320655822754 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0788\n",
            "Epoch 40 Batch 0 Loss 0.0975\n",
            "Epoch 40 Loss 0.1115\n",
            "Time taken for 1 epoch 22.810823440551758 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.1201\n",
            "Epoch 42 Batch 0 Loss 0.0938\n",
            "Epoch 42 Loss 0.1124\n",
            "Time taken for 1 epoch 21.715707063674927 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0746\n",
            "Epoch 44 Batch 0 Loss 0.1014\n",
            "Epoch 44 Loss 0.1107\n",
            "Time taken for 1 epoch 21.700387716293335 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0729\n",
            "Epoch 46 Batch 0 Loss 0.0969\n",
            "Epoch 46 Loss 0.1098\n",
            "Time taken for 1 epoch 21.70632243156433 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0644\n",
            "Epoch 48 Batch 0 Loss 0.1010\n",
            "Epoch 48 Loss 0.1085\n",
            "Time taken for 1 epoch 21.489150047302246 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0942\n",
            "Epoch 50 Batch 0 Loss 0.0946\n",
            "Epoch 50 Loss 0.1051\n",
            "Time taken for 1 epoch 21.48416042327881 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0731\n",
            "Epoch 52 Batch 0 Loss 0.0985\n",
            "Epoch 52 Loss 0.1062\n",
            "Time taken for 1 epoch 21.58918833732605 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0749\n",
            "Epoch 54 Batch 0 Loss 0.0929\n",
            "Epoch 54 Loss 0.1037\n",
            "Time taken for 1 epoch 21.39253830909729 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.1090\n",
            "Epoch 56 Batch 0 Loss 0.0813\n",
            "Epoch 56 Loss 0.1030\n",
            "Time taken for 1 epoch 21.49638032913208 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0905\n",
            "Epoch 58 Batch 0 Loss 0.0922\n",
            "Epoch 58 Loss 0.1004\n",
            "Time taken for 1 epoch 21.386088609695435 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0884\n",
            "Epoch 60 Batch 0 Loss 0.0930\n",
            "Epoch 60 Loss 0.1022\n",
            "Time taken for 1 epoch 21.316975831985474 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0749\n",
            "Epoch 62 Batch 0 Loss 0.0735\n",
            "Epoch 62 Loss 0.1019\n",
            "Time taken for 1 epoch 21.69747495651245 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1033\n",
            "Epoch 64 Batch 0 Loss 0.0936\n",
            "Epoch 64 Loss 0.0990\n",
            "Time taken for 1 epoch 21.617092609405518 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0899\n",
            "Epoch 66 Batch 0 Loss 0.0904\n",
            "Epoch 66 Loss 0.0977\n",
            "Time taken for 1 epoch 22.01296877861023 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0983\n",
            "Epoch 68 Batch 0 Loss 0.0867\n",
            "Epoch 68 Loss 0.1005\n",
            "Time taken for 1 epoch 21.733680963516235 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0667\n",
            "Epoch 70 Batch 0 Loss 0.0957\n",
            "Epoch 70 Loss 0.0996\n",
            "Time taken for 1 epoch 22.59416627883911 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0697\n",
            "Epoch 72 Batch 0 Loss 0.0593\n",
            "Epoch 72 Loss 0.1002\n",
            "Time taken for 1 epoch 21.56639552116394 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0799\n",
            "Epoch 74 Batch 0 Loss 0.0851\n",
            "Epoch 74 Loss 0.1000\n",
            "Time taken for 1 epoch 21.756348609924316 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0851\n",
            "Epoch 76 Batch 0 Loss 0.1012\n",
            "Epoch 76 Loss 0.0961\n",
            "Time taken for 1 epoch 21.72069764137268 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0946\n",
            "Epoch 78 Batch 0 Loss 0.0751\n",
            "Epoch 78 Loss 0.0985\n",
            "Time taken for 1 epoch 21.68082904815674 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0925\n",
            "Epoch 80 Batch 0 Loss 0.0630\n",
            "Epoch 80 Loss 0.0968\n",
            "Time taken for 1 epoch 22.2821044921875 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0866\n",
            "Epoch 82 Batch 0 Loss 0.0872\n",
            "Epoch 82 Loss 0.0982\n",
            "Time taken for 1 epoch 21.659641981124878 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0529\n",
            "Epoch 84 Batch 0 Loss 0.0796\n",
            "Epoch 84 Loss 0.0975\n",
            "Time taken for 1 epoch 22.08912491798401 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0844\n",
            "Epoch 86 Batch 0 Loss 0.0913\n",
            "Epoch 86 Loss 0.0973\n",
            "Time taken for 1 epoch 21.790377140045166 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0833\n",
            "Epoch 88 Batch 0 Loss 0.0788\n",
            "Epoch 88 Loss 0.0982\n",
            "Time taken for 1 epoch 21.620447158813477 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0925\n",
            "Epoch 90 Batch 0 Loss 0.0770\n",
            "Epoch 90 Loss 0.0983\n",
            "Time taken for 1 epoch 21.842660188674927 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0839\n",
            "Epoch 92 Batch 0 Loss 0.0642\n",
            "Epoch 92 Loss 0.0984\n",
            "Time taken for 1 epoch 21.70088529586792 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0786\n",
            "Epoch 94 Batch 0 Loss 0.0972\n",
            "Epoch 94 Loss 0.0956\n",
            "Time taken for 1 epoch 21.986305952072144 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0917\n",
            "Epoch 96 Batch 0 Loss 0.0778\n",
            "Epoch 96 Loss 0.0951\n",
            "Time taken for 1 epoch 21.550392627716064 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0844\n",
            "Epoch 98 Batch 0 Loss 0.0757\n",
            "Epoch 98 Loss 0.0955\n",
            "Time taken for 1 epoch 21.627037048339844 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0655\n",
            "Epoch 100 Batch 0 Loss 0.0798\n",
            "Epoch 100 Loss 0.0961\n",
            "Time taken for 1 epoch 22.441532373428345 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybmmiLecY5X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycg_V79-Y5X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73f0a6d7-dc2d-4187-b36b-ee2ed8601884"
      },
      "source": [
        "# load checklist object from saving\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5a0e9b4908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOEHCtTBY5X7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad0181d7-fc40-491e-c87f-8e7d3d525ba6"
      },
      "source": [
        "translate(\"where you go!\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> where you go ! <end>\n",
            "Predicted translation: আপনি কোথায় ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNVFHJE8KMl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0c88100c-136a-4bb8-f441-c4fcd2415b64"
      },
      "source": [
        "translate(\"get well\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> get well <end>\n",
            "Predicted translation: বেরও । <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpyYHfHmKqUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "31d080af-d7ce-48a1-f2c1-a9658c3da2f4"
      },
      "source": [
        "translate(\" let me leave!\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> let me leave ! <end>\n",
            "Predicted translation: আমাকে যেতে দাও ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
