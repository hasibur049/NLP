{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complex_type_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasiburrahman1/NLP/blob/master/complex_type_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EIq8vUx7Xyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3058042a-3caf-4780-ba55-ce60a7315c04"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zESkDF3bbokn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as et \n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, SimpleRNN, GRU, LSTM, Bidirectional, Dropout, Input, Conv2D, MaxPool2D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input, GlobalMaxPooling1D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJaUPyNVczAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import xlrd\n",
        "import csv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNe1S5OIb5ig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0eb5ba5b-7b7b-42de-d116-b10828800b3d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDEotyU0eK1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32c029ca-b760-4849-a497-24fae1e6368e"
      },
      "source": [
        "'''\n",
        "cols_to_use = ['Question', 'Answer'] # or [0,1,2,3]\n",
        "df = pd.read_csv('data.csv', usecols= cols_to_use)\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncols_to_use = ['Question', 'Answer'] # or [0,1,2,3]\\ndf = pd.read_csv('data.csv', usecols= cols_to_use)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PippEx4sk81E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b63325eb-8d72-4445-8f15-fbf56440200a"
      },
      "source": [
        "'''\n",
        "mport pandas\n",
        "df = pandas.read_excel(r\"/test.xlsx\")\n",
        "df = df.drop('label', 1)\n",
        "#print the column names\n",
        "print(df.columns)\n",
        "\n",
        "#get the values for a given column\n",
        "#values = df['collumn_name'].values\n",
        "\n",
        "#get a data frame with selected columns\n",
        "FORMAT = ['Question', 'Answer']\n",
        "df_selected = df[FORMAT]\n",
        "\n",
        "\n",
        "df.to_csv('dataset_csv.csv')\n",
        "!cp dataset_csv.csv \"drive/My Drive/\"\n",
        "\n",
        "df.describe()\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmport pandas\\ndf = pandas.read_excel(r\"/test.xlsx\")\\ndf = df.drop(\\'label\\', 1)\\n#print the column names\\nprint(df.columns)\\n\\n#get the values for a given column\\n#values = df[\\'collumn_name\\'].values\\n\\n#get a data frame with selected columns\\nFORMAT = [\\'Question\\', \\'Answer\\']\\ndf_selected = df[FORMAT]\\n\\n\\ndf.to_csv(\\'dataset_csv.csv\\')\\n!cp dataset_csv.csv \"drive/My Drive/\"\\n\\ndf.describe()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BClueJRP0GrD",
        "colab_type": "text"
      },
      "source": [
        "---------------------------**Start**-------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArvtQm2Ar4ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset path\n",
        "#path_to_file = \"/datasetCleaned.txt\"\n",
        "\n",
        "path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/QA_complex_type.txt\"\n",
        "#path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/test_Text_Document.txt\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zxeGMUjieLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhJ9kSaxz9ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "#     remove punctuations from lines\n",
        "    w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    #print(w)\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    #print(w)\n",
        "    return w"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBJnP9_Az42-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabularyCounter(dataQuestion, dataAnswer):\n",
        "    # setup Vocabulary \n",
        "    question_words = collections.Counter([word for sentence in dataQuestion for word in sentence.split()])\n",
        "    answer_words = collections.Counter([word for sentence in dataAnswer for word in sentence.split()])\n",
        "    \n",
        "    print('Total English words: {}'.format(len([word for sentence in dataQuestion for word in sentence.split()])))\n",
        "    print('Unique English words: {}'.format(len(question_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*question_words.most_common(10)))[0]) + '\"\\n')\n",
        "\n",
        "    print('Total Bangla words: {}'.format(len([word for sentence in dataAnswer for word in sentence.split()])))\n",
        "    print('Unique Bangla words: {}'.format(len(answer_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*answer_words.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed6ZnaGgcYwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, BANGLA]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    \"\"\"\n",
        "    test test test\n",
        "    \"\"\"\n",
        "    #print(lines)\n",
        "    #print(\"\\n\")\n",
        "    #print(num_examples)\n",
        "    \"\"\"\n",
        "    test test test\n",
        "    \"\"\"\n",
        "\n",
        "    question = []\n",
        "    answer = []\n",
        "    for l in lines[:num_examples]:\n",
        "        #print(l)\n",
        "        w = l.split('\\t')\n",
        "        \"\"\"\n",
        "        test test test\n",
        "        \"\"\"\n",
        "        #print(w)\n",
        "        #print(\"\\n\")\n",
        "        \"\"\"\n",
        "        test test test\n",
        "        \"\"\"\n",
        "        question.append(w[0])\n",
        "        answer.append(w[1])\n",
        "    vocabularyCounter(question, answer) \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    #print(word_pairs)\n",
        "#     rearrange dataset src and target\n",
        "    for indd in range(len(word_pairs)):\n",
        "        temp = word_pairs[indd][0]\n",
        "        word_pairs[indd][0] = word_pairs[indd][1]\n",
        "        word_pairs[indd][1] = temp\n",
        "    #print(word_pairs)\n",
        "    return zip(*word_pairs)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D03Ndl35ceWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "e6bab9e0-4605-4647-e86e-e0326961f041"
      },
      "source": [
        "# preprocessing function declear and vocabulary checking\n",
        "bn, en = create_dataset(path_to_file, None)\n",
        "\n",
        "#print(\"\\n\\n\", en[4])\n",
        "#print(bn)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 877\n",
            "Unique English words: 420\n",
            "Most Common Words: \"the\" \"is\" \"of\" \"\"Which\" \"in\" \"\"What\" \"are\" \"\"Is\" \"with\" \"protein\"\n",
            "\n",
            "Total Bangla words: 2984\n",
            "Unique Bangla words: 1454\n",
            "Most Common Words: \"of\" \"the\" \"and\" \",\" \"in\" \"to\" \".\"\" \"(\" \")\" \"a\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffe8Sh7Pjm5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3fe48afb-7b44-4b44-8f90-b78b923e4832"
      },
      "source": [
        "#function declearation for \n",
        "#     1. length of sentence\n",
        "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
        "#https://stackoverflow.com/questions/61661160/comparison-of-tf-keras-preprocessing-text-tokenizer-and-tfds-features-text-tok\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#     2. tokenizer\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "#     3. load dataset\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang  = create_dataset(path, num_examples)\n",
        "    \n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "'''\n",
        "   Total English words: 40\n",
        "Unique English words: 9\n",
        "Most Common Words: \"Do\" \"lincRNAs\" \"play\" \"a\" \"role\" \"human\" \"cancer?\" \"1in\" \"in\"\n",
        "\n",
        "Total Bangla words: 66\n",
        "Unique Bangla words: 50\n",
        "Most Common Words: \"lincRNAs\" \"of\" \"and\" \"to\" \"polyadenylation\" \"in\" \"cancer\" \"we\" \"the\" \"Genome-wide\"\n",
        "(array([[ 1,  2,  3,  4,  5,  6, 12,  7,  8,  9, 10],\n",
        "        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\n",
        "        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\n",
        "        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\n",
        "        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10]], dtype=int32),\n",
        " array([[ 1, 13, 14,  7, 15, 16,  5,  2,  8,  9, 10, 17,  3,  4,  0,  0,\n",
        "          0,  0,  0,  0],\n",
        "        [ 1, 18, 19, 11, 20,  2,  6, 21, 22,  9, 23, 24,  5, 25, 26,  3,\n",
        "          4,  0,  0,  0],\n",
        "        [ 1, 27, 28,  5,  2, 29, 30, 31, 32, 33, 34,  3,  4,  0,  0,  0,\n",
        "          0,  0,  0,  0],\n",
        "        [ 1,  6, 35, 12, 36,  5,  2,  8, 37,  6, 10, 38, 39, 40, 41,  3,\n",
        "          4,  0,  0,  0],\n",
        "        [ 1, 42, 11, 43, 44, 45, 46,  7, 47, 48,  6, 49, 12, 50, 51,  2,\n",
        "          7, 52,  3,  4]], dtype=int32),\n",
        " <keras_preprocessing.text.Tokenizer at 0x7f9045bf34a8>,\n",
        " <keras_preprocessing.text.Tokenizer at 0x7f9045d264a8>)\n",
        " ''' "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n   Total English words: 40\\nUnique English words: 9\\nMost Common Words: \"Do\" \"lincRNAs\" \"play\" \"a\" \"role\" \"human\" \"cancer?\" \"1in\" \"in\"\\n\\nTotal Bangla words: 66\\nUnique Bangla words: 50\\nMost Common Words: \"lincRNAs\" \"of\" \"and\" \"to\" \"polyadenylation\" \"in\" \"cancer\" \"we\" \"the\" \"Genome-wide\"\\n(array([[ 1,  2,  3,  4,  5,  6, 12,  7,  8,  9, 10],\\n        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\\n        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\\n        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10],\\n        [ 1,  2,  3,  4,  5,  6, 11,  7,  8,  9, 10]], dtype=int32),\\n array([[ 1, 13, 14,  7, 15, 16,  5,  2,  8,  9, 10, 17,  3,  4,  0,  0,\\n          0,  0,  0,  0],\\n        [ 1, 18, 19, 11, 20,  2,  6, 21, 22,  9, 23, 24,  5, 25, 26,  3,\\n          4,  0,  0,  0],\\n        [ 1, 27, 28,  5,  2, 29, 30, 31, 32, 33, 34,  3,  4,  0,  0,  0,\\n          0,  0,  0,  0],\\n        [ 1,  6, 35, 12, 36,  5,  2,  8, 37,  6, 10, 38, 39, 40, 41,  3,\\n          4,  0,  0,  0],\\n        [ 1, 42, 11, 43, 44, 45, 46,  7, 47, 48,  6, 49, 12, 50, 51,  2,\\n          7, 52,  3,  4]], dtype=int32),\\n <keras_preprocessing.text.Tokenizer at 0x7f9045bf34a8>,\\n <keras_preprocessing.text.Tokenizer at 0x7f9045d264a8>)\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4m8uImymjec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4f4fa8f6-cc72-498c-816b-f5b92908e076"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "\n",
        "print(input_tensor)\n",
        "print(\"\\n\")\n",
        "print(inp_lang)\n",
        "\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(\"Size:- Train_X-\", len(input_tensor_train), \"Train_Y-\", len(target_tensor_train), \"Test_X-\", len(input_tensor_val), \"Test_Y-\", len(target_tensor_val))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 877\n",
            "Unique English words: 420\n",
            "Most Common Words: \"the\" \"is\" \"of\" \"\"Which\" \"in\" \"\"What\" \"are\" \"\"Is\" \"with\" \"protein\"\n",
            "\n",
            "Total Bangla words: 2984\n",
            "Unique Bangla words: 1454\n",
            "Most Common Words: \"of\" \"the\" \"and\" \",\" \"in\" \"to\" \".\"\" \"(\" \")\" \"a\"\n",
            "[[ 1 43 89 ...  0  0  0]\n",
            " [ 1  9  4 ...  0  0  0]\n",
            " [ 1  6 31 ...  0  0  0]\n",
            " ...\n",
            " [ 1  9  4 ...  0  0  0]\n",
            " [ 1  9  4 ...  0  0  0]\n",
            " [ 1  9  4 ...  0  0  0]]\n",
            "\n",
            "\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7fd6b5a904e0>\n",
            "Size:- Train_X- 80 Train_Y- 80 Test_X- 20 Test_Y- 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAZehOA41iIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "48e8ddcb-a9c2-4a62-c03a-4fb6f5be5856"
      },
      "source": [
        "# showing example of tokenization\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "   # print(\"t t t\",t)\n",
        "\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "6 ----> which\n",
            "81 ----> receptors\n",
            "10 ----> are\n",
            "36 ----> targeted\n",
            "19 ----> by\n",
            "59 ----> suvorexant\n",
            "3 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "4 ----> <start>\n",
            "1069 ----> medicinal\n",
            "1070 ----> chemistry\n",
            "1071 ----> efforts\n",
            "1072 ----> focused\n",
            "25 ----> on\n",
            "1 ----> the\n",
            "1073 ----> reduction\n",
            "2 ----> of\n",
            "1074 ----> bioactivation\n",
            "294 ----> potential\n",
            "2 ----> of\n",
            "1075 ----> diazepane\n",
            "1076 ----> amide\n",
            "36 ----> 1\n",
            "90 ----> through\n",
            "1 ----> the\n",
            "1077 ----> modification\n",
            "2 ----> of\n",
            "1 ----> the\n",
            "1078 ----> western\n",
            "1079 ----> heterocycle\n",
            "295 ----> resulted\n",
            "7 ----> in\n",
            "1 ----> the\n",
            "1080 ----> discovery\n",
            "2 ----> of\n",
            "1081 ----> suvorexant\n",
            "10 ----> a\n",
            "1082 ----> dora\n",
            "101 ----> recently\n",
            "1083 ----> approved\n",
            "17 ----> by\n",
            "1 ----> the\n",
            "1084 ----> fda\n",
            "14 ----> for\n",
            "1 ----> the\n",
            "43 ----> treatment\n",
            "2 ----> of\n",
            "1085 ----> insomnia\n",
            "5 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM6ecPNLD7DY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "537a5267-5334-41d1-9efd-d7f443b5c8e7"
      },
      "source": [
        "#https://www.geeksforgeeks.org/tensorflow-tf-data-dataset-from_tensor_slices/\n",
        "#https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset\n",
        "\n",
        "BUFFER_SIZE = len(target_tensor_train)\n",
        "#print(BUFFER_SIZE)\n",
        "BATCH_SIZE = 64\n",
        "print(\"\\n\")\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "#print(steps_per_epoch)\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "print(dataset)\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print(\"Input-output shape batch\", example_input_batch.shape, example_target_batch.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "<ShuffleDataset shapes: ((19,), (76,)), types: (tf.int32, tf.int32)>\n",
            "<BatchDataset shapes: ((64, 19), (64, 76)), types: (tf.int32, tf.int32)>\n",
            "Input-output shape batch (64, 19) (64, 76)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnpOSuQdFOmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "6ffdd37e-5477-445b-c831-08afe893686b"
      },
      "source": [
        "print(input_tensor_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1   6  81 ...   0   0   0]\n",
            " [  1   6  14 ...   2   0   0]\n",
            " [  1   9   4 ...   0   0   0]\n",
            " ...\n",
            " [  1 245 246 ...   0   0   0]\n",
            " [  1  10 339 ...   0   0   0]\n",
            " [  1   6  10 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYr7Pcqa4lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7dcf1e11-fca3-4dd6-c0ae-31623dd0d05d"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, activation='linear',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 19, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGx5l5Y4cBdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a5617909-268c-4a12-c437-a0e28d920885"
      },
      "source": [
        "# Attention and context_vector\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.sigmoid(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    attention_weights = tf.nn.sigmoid(score)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 19, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysBOqXxOcYWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5ec0534-4ebb-4e93-d88e-a9ee94b3a91f"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, activation='tanh',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 1399)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6jYV4yc9Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6kC09H7dSSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoint for saving the object\n",
        "checkpoint_dir = './training_checkpoints_saved'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpointSaved\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEZROaj3dW2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQNXUw6db0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9be1565-0fd2-44a9-d3de-4fcda8f4bf14"
      },
      "source": [
        "# start learn processing\n",
        "EPOCHS = 100\n",
        "lossMatrix = []\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    lossMatrix.append(total_loss / steps_per_epoch)\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.0110\n",
            "Epoch 2 Batch 0 Loss 3.0290\n",
            "Epoch 2 Loss 3.0290\n",
            "Time taken for 1 epoch 19.634249210357666 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.6875\n",
            "Epoch 4 Batch 0 Loss 2.8658\n",
            "Epoch 4 Loss 2.8658\n",
            "Time taken for 1 epoch 19.585787057876587 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.6459\n",
            "Epoch 6 Batch 0 Loss 2.5041\n",
            "Epoch 6 Loss 2.5041\n",
            "Time taken for 1 epoch 19.639033794403076 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.6603\n",
            "Epoch 8 Batch 0 Loss 2.6514\n",
            "Epoch 8 Loss 2.6514\n",
            "Time taken for 1 epoch 19.50509476661682 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5506\n",
            "Epoch 10 Batch 0 Loss 2.5588\n",
            "Epoch 10 Loss 2.5588\n",
            "Time taken for 1 epoch 19.494422912597656 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.5284\n",
            "Epoch 12 Batch 0 Loss 2.5942\n",
            "Epoch 12 Loss 2.5942\n",
            "Time taken for 1 epoch 19.46480107307434 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.5452\n",
            "Epoch 14 Batch 0 Loss 2.4821\n",
            "Epoch 14 Loss 2.4821\n",
            "Time taken for 1 epoch 19.52499222755432 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.3882\n",
            "Epoch 16 Batch 0 Loss 2.4938\n",
            "Epoch 16 Loss 2.4938\n",
            "Time taken for 1 epoch 20.399035930633545 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.4964\n",
            "Epoch 18 Batch 0 Loss 2.5346\n",
            "Epoch 18 Loss 2.5346\n",
            "Time taken for 1 epoch 19.41508150100708 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.5351\n",
            "Epoch 20 Batch 0 Loss 2.4433\n",
            "Epoch 20 Loss 2.4433\n",
            "Time taken for 1 epoch 19.406307697296143 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 2.4810\n",
            "Epoch 22 Batch 0 Loss 2.4741\n",
            "Epoch 22 Loss 2.4741\n",
            "Time taken for 1 epoch 19.597281455993652 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 2.3388\n",
            "Epoch 24 Batch 0 Loss 2.3988\n",
            "Epoch 24 Loss 2.3988\n",
            "Time taken for 1 epoch 19.623945474624634 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 2.4126\n",
            "Epoch 26 Batch 0 Loss 2.4111\n",
            "Epoch 26 Loss 2.4111\n",
            "Time taken for 1 epoch 19.414122104644775 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 2.3200\n",
            "Epoch 28 Batch 0 Loss 2.3769\n",
            "Epoch 28 Loss 2.3769\n",
            "Time taken for 1 epoch 19.469188690185547 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 2.3412\n",
            "Epoch 30 Batch 0 Loss 2.2559\n",
            "Epoch 30 Loss 2.2559\n",
            "Time taken for 1 epoch 19.51694107055664 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 2.1640\n",
            "Epoch 32 Batch 0 Loss 2.1567\n",
            "Epoch 32 Loss 2.1567\n",
            "Time taken for 1 epoch 19.572456121444702 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 2.0898\n",
            "Epoch 34 Batch 0 Loss 2.1276\n",
            "Epoch 34 Loss 2.1276\n",
            "Time taken for 1 epoch 19.447858095169067 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 2.0343\n",
            "Epoch 36 Batch 0 Loss 2.0584\n",
            "Epoch 36 Loss 2.0584\n",
            "Time taken for 1 epoch 19.595906972885132 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.9423\n",
            "Epoch 38 Batch 0 Loss 1.9395\n",
            "Epoch 38 Loss 1.9395\n",
            "Time taken for 1 epoch 19.544711589813232 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.9412\n",
            "Epoch 40 Batch 0 Loss 1.8937\n",
            "Epoch 40 Loss 1.8937\n",
            "Time taken for 1 epoch 19.396788597106934 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.8239\n",
            "Epoch 42 Batch 0 Loss 1.7765\n",
            "Epoch 42 Loss 1.7765\n",
            "Time taken for 1 epoch 19.369508981704712 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 1.7461\n",
            "Epoch 44 Batch 0 Loss 1.6891\n",
            "Epoch 44 Loss 1.6891\n",
            "Time taken for 1 epoch 19.471620798110962 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 1.7136\n",
            "Epoch 46 Batch 0 Loss 1.5645\n",
            "Epoch 46 Loss 1.5645\n",
            "Time taken for 1 epoch 19.41864824295044 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 1.5196\n",
            "Epoch 48 Batch 0 Loss 1.5237\n",
            "Epoch 48 Loss 1.5237\n",
            "Time taken for 1 epoch 19.94193959236145 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 1.4664\n",
            "Epoch 50 Batch 0 Loss 1.4651\n",
            "Epoch 50 Loss 1.4651\n",
            "Time taken for 1 epoch 19.502393007278442 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 1.3500\n",
            "Epoch 52 Batch 0 Loss 1.3420\n",
            "Epoch 52 Loss 1.3420\n",
            "Time taken for 1 epoch 19.505046367645264 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 1.2862\n",
            "Epoch 54 Batch 0 Loss 1.3047\n",
            "Epoch 54 Loss 1.3047\n",
            "Time taken for 1 epoch 19.52604913711548 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 1.2365\n",
            "Epoch 56 Batch 0 Loss 1.2253\n",
            "Epoch 56 Loss 1.2253\n",
            "Time taken for 1 epoch 19.501302480697632 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 1.1035\n",
            "Epoch 58 Batch 0 Loss 1.1482\n",
            "Epoch 58 Loss 1.1482\n",
            "Time taken for 1 epoch 19.538777351379395 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 1.0848\n",
            "Epoch 60 Batch 0 Loss 0.9936\n",
            "Epoch 60 Loss 0.9936\n",
            "Time taken for 1 epoch 19.487061738967896 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.9154\n",
            "Epoch 62 Batch 0 Loss 0.9786\n",
            "Epoch 62 Loss 0.9786\n",
            "Time taken for 1 epoch 19.615195274353027 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.9446\n",
            "Epoch 64 Batch 0 Loss 0.8740\n",
            "Epoch 64 Loss 0.8740\n",
            "Time taken for 1 epoch 19.483125686645508 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.8253\n",
            "Epoch 66 Batch 0 Loss 0.8135\n",
            "Epoch 66 Loss 0.8135\n",
            "Time taken for 1 epoch 19.477198600769043 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.7437\n",
            "Epoch 68 Batch 0 Loss 0.7328\n",
            "Epoch 68 Loss 0.7328\n",
            "Time taken for 1 epoch 19.50322675704956 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.7145\n",
            "Epoch 70 Batch 0 Loss 0.6965\n",
            "Epoch 70 Loss 0.6965\n",
            "Time taken for 1 epoch 19.659645080566406 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.6460\n",
            "Epoch 72 Batch 0 Loss 0.6157\n",
            "Epoch 72 Loss 0.6157\n",
            "Time taken for 1 epoch 19.55012035369873 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.6243\n",
            "Epoch 74 Batch 0 Loss 0.5840\n",
            "Epoch 74 Loss 0.5840\n",
            "Time taken for 1 epoch 19.595532178878784 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.5418\n",
            "Epoch 76 Batch 0 Loss 0.5090\n",
            "Epoch 76 Loss 0.5090\n",
            "Time taken for 1 epoch 19.55075192451477 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.4995\n",
            "Epoch 78 Batch 0 Loss 0.4793\n",
            "Epoch 78 Loss 0.4793\n",
            "Time taken for 1 epoch 19.495200634002686 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.4869\n",
            "Epoch 80 Batch 0 Loss 0.4751\n",
            "Epoch 80 Loss 0.4751\n",
            "Time taken for 1 epoch 19.58363652229309 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.4247\n",
            "Epoch 82 Batch 0 Loss 0.4205\n",
            "Epoch 82 Loss 0.4205\n",
            "Time taken for 1 epoch 19.461399793624878 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.4099\n",
            "Epoch 84 Batch 0 Loss 0.3880\n",
            "Epoch 84 Loss 0.3880\n",
            "Time taken for 1 epoch 19.51540470123291 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.3743\n",
            "Epoch 86 Batch 0 Loss 0.3519\n",
            "Epoch 86 Loss 0.3519\n",
            "Time taken for 1 epoch 19.570743322372437 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.3431\n",
            "Epoch 88 Batch 0 Loss 0.3264\n",
            "Epoch 88 Loss 0.3264\n",
            "Time taken for 1 epoch 19.472909450531006 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.3132\n",
            "Epoch 90 Batch 0 Loss 0.2957\n",
            "Epoch 90 Loss 0.2957\n",
            "Time taken for 1 epoch 19.562017679214478 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.2878\n",
            "Epoch 92 Batch 0 Loss 0.2710\n",
            "Epoch 92 Loss 0.2710\n",
            "Time taken for 1 epoch 19.496704578399658 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.2575\n",
            "Epoch 94 Batch 0 Loss 0.2380\n",
            "Epoch 94 Loss 0.2380\n",
            "Time taken for 1 epoch 19.60903549194336 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.2459\n",
            "Epoch 96 Batch 0 Loss 0.2226\n",
            "Epoch 96 Loss 0.2226\n",
            "Time taken for 1 epoch 19.58170247077942 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.2212\n",
            "Epoch 98 Batch 0 Loss 0.2162\n",
            "Epoch 98 Loss 0.2162\n",
            "Time taken for 1 epoch 19.695770263671875 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.2146\n",
            "Epoch 100 Batch 0 Loss 0.2019\n",
            "Epoch 100 Loss 0.2019\n",
            "Time taken for 1 epoch 19.588736057281494 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGQwubxgdet8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        " \n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtKzcpbtdhO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67f26cad-c908-473b-96da-5e75687ef40d"
      },
      "source": [
        "# load checklist object from saving\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd6ae4b4c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar5roHQGQzcY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY8zGRwndj35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7abf3c91-e726-4c4a-e5e1-b36788881d0b"
      },
      "source": [
        "translate(\" what is cancer?\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what is cancer ? <end>\n",
            "Predicted translation: genome-wide hormone-dependent enhancer-activity maps . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtX6zc17QnTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b9549738-7c3b-4a2c-b84e-f8af65086af9"
      },
      "source": [
        "translate(\" do lincrnas play a role in human cancer?\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> do lincrnas play a role in human cancer ? <end>\n",
            "Predicted translation: genome-wide identification and predictive modeling of lincrnas polyadenylation in cancer genome <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmjDV-sYRmg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fe8500c6-4838-4f56-bec8-78e1b64c14e9"
      },
      "source": [
        "translate(\" what play a role in human cancer?\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what play a role in human cancer ? <end>\n",
            "Predicted translation: genome-wide identification and predictive modeling of lincrnas polyadenylation in cancer genome <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3glRrKcR5cC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d97009d9-bb9d-4086-bf49-100bc87e549a"
      },
      "source": [
        "translate(\" what is human cancer?\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what is human cancer ? <end>\n",
            "Predicted translation: genome-wide hormone-dependent enhancer-activity maps . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucUxVznuSjsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d97141ce-7ec8-417d-8a75-1faf78de13b0"
      },
      "source": [
        "translate(\" is endostatin a proangiogenic factor? \")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> is endostatin a proangiogenic factor ? <end>\n",
            "Predicted translation: an imbalance between vegf and endostatin underlies impaired angiogenesis in gastric_mucosa of aging rats <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx9P_jlJSxxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b99b3d4d-608c-4574-8386-c4ef0646e891"
      },
      "source": [
        "translate(\" what is proangiogenic factor? \")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what is proangiogenic factor ? <end>\n",
            "Predicted translation: we used starr-seq , and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two different an imbalance between vegf and endostatin underlies impaired in two \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipo5-rssTGBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0f6ca668-ec9f-4f8a-9631-4cdf993cb7ae"
      },
      "source": [
        "translate(\" list two common features of tay syndrome.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> list two common features of tay syndrome . <end>\n",
            "Predicted translation: the tay syndrome ( congenital ichthyosis with trichothiodystrophy ) . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}