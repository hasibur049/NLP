{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MachineTranslationEn-Bn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasiburrahman1/NLP/blob/master/MachineTranslationEn_Bn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suc7S5p4Y2K1",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation English to Bangla with RNN\n",
        "\n",
        "Machine Translation using Recurrent Neural Network\n",
        "> License https://www.apache.org/licenses/LICENSE-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4BD5nEY2K2",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "- Install Anaconda `Jupyter` Notebook.\n",
        "- Install Python3.\n",
        "- With Anaconda Prompt install `Tensorflow Keras` NLP Tools. For Faster Processing need GPU processing. \n",
        "    - Keras\n",
        "```batch\n",
        "   conda install -c conda-forge tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOxERuyY2K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFx5duEDY2K5",
        "colab_type": "code",
        "colab": {},
        "outputId": "d17703dd-d5e4-4070-feeb-0b1eca33ff86"
      },
      "source": [
        "# check up GPU/CPU processing power\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6515303086157523200\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 18017270970326380120\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY0gJBg5Y2K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset path\n",
        "path_to_file = \"data/datasetCleaned.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMEPdekY2K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "#     remove punctuations from lines\n",
        "    w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg9zGXvXY2K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabularyCounter(dataFileEn, dataFileBn):\n",
        "    # setup Vocabulary \n",
        "    english_words = collections.Counter([word for sentence in dataFileEn for word in sentence.split()])\n",
        "    bangla_words = collections.Counter([word for sentence in dataFileBn for word in sentence.split()])\n",
        "    \n",
        "    print('Total English words: {}'.format(len([word for sentence in dataFileEn for word in sentence.split()])))\n",
        "    print('Unique English words: {}'.format(len(english_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*english_words.most_common(10)))[0]) + '\"\\n')\n",
        "\n",
        "    print('Total Bangla words: {}'.format(len([word for sentence in dataFileBn for word in sentence.split()])))\n",
        "    print('Unique Bangla words: {}'.format(len(bangla_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*bangla_words.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvPtcrZHY2LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, BANGLA]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    allEnLines = []\n",
        "    allBnLines = []\n",
        "    for l in lines[:num_examples]:\n",
        "        w = l.split('\\t')\n",
        "        allEnLines.append(w[0])\n",
        "        allBnLines.append(w[1])\n",
        "    vocabularyCounter(allEnLines, allBnLines) \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "#     rearrange dataset src and target\n",
        "    for indd in range(len(word_pairs)):\n",
        "        temp = word_pairs[indd][0]\n",
        "        word_pairs[indd][0] = word_pairs[indd][1]\n",
        "        word_pairs[indd][1] = temp\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKh4AJwlY2LD",
        "colab_type": "code",
        "colab": {},
        "outputId": "8cc417bb-db5c-415e-db2a-ac27d186d89e"
      },
      "source": [
        "# preprocessing function declear and vocabulary checking\n",
        "bn, en = create_dataset(path_to_file, None)\n",
        "print(\"\\n\\n\", en[20])\n",
        "print(bn[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 19606\n",
            "Unique English words: 2839\n",
            "Most Common Words: \"I\" \"Tom\" \"to\" \"you\" \"is\" \"the\" \"a\" \"I'm\" \"your\" \"He\"\n",
            "\n",
            "Total Bangla words: 19000\n",
            "Unique Bangla words: 3527\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"কি\" \"না।\" \"আপনি\" \"এটা\" \"তুমি\" \"কথা\" \"আপনার\"\n",
            "\n",
            "\n",
            " <start> Got it ! <end>\n",
            "<start> বুঝে গেছি ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbQvbhtxY2LG",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g09tm_SdY2LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function declearation for \n",
        "#     1. length of sentence\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#     2. tokenizer\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "#     3. load dataset\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang  = create_dataset(path, num_examples)\n",
        "    \n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BIYv6rSY2LI",
        "colab_type": "text"
      },
      "source": [
        "## Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0mtTK7iY2LJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1ef0920-1aa2-43b3-a19b-5dc7f4f3d045"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(\"Size:- Train_X-\", len(input_tensor_train), \"Train_Y-\", len(target_tensor_train), \"Test_X-\", len(input_tensor_val), \"Test_Y-\", len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 2425\n",
            "Unique English words: 555\n",
            "Most Common Words: \"I\" \"Tom\" \"Tom.\" \"I'm\" \"is\" \"He\" \"Who\" \"You\" \"me.\" \"it.\"\n",
            "\n",
            "Total Bangla words: 2645\n",
            "Unique Bangla words: 770\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"টমকে\" \"আমরা\" \"আমাকে\" \"না।\" \"এটা\" \"কি\" \"করুন।\"\n",
            "Size:- Train_X- 800 Train_Y- 800 Test_X- 200 Test_Y- 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqv2rn3EY2LL",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d073e7f-293f-4b29-ffde-8c6e3dd39299"
      },
      "source": [
        "# showing example of tokenization\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "209 ----> love\n",
            "7 ----> you\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> আমি\n",
            "78 ----> তোমাকে\n",
            "483 ----> ভালবাসি\n",
            "3 ----> ।\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDF5fzc6Y2LN",
        "colab_type": "code",
        "colab": {},
        "outputId": "7329d817-b464-4c6d-b818-43cd9f4be4c6"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print(\"Input-output shape batch\", example_input_batch.shape, example_target_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input-output shape batch (64, 7) (64, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZyxu2jTY2LP",
        "colab_type": "code",
        "colab": {},
        "outputId": "60758391-8797-4648-9019-a22eb2280731"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, activation='linear',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 7, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isEATm0WY2LR",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8f6c805-49ee-4b23-ad2a-071cc7919931"
      },
      "source": [
        "# Attention and context_vector\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.sigmoid(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    attention_weights = tf.nn.sigmoid(score)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQwuqvxOY2LT",
        "colab_type": "code",
        "colab": {},
        "outputId": "32d36999-a348-46da-fe94-1f090ed97a99"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, activation='tanh',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 673)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGqLe-DTY2LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE65JaBJY2LX",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoint saving object "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbW_OnVKY2LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoint for saving the object\n",
        "checkpoint_dir = './training_checkpoints_saved'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpointSaved\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5oaYXIMY2LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjxUeFuY2Ld",
        "colab_type": "code",
        "colab": {},
        "outputId": "73445f93-fea4-422c-cfa2-ae732a7e6acc"
      },
      "source": [
        "# start learn processing\n",
        "EPOCHS = 100\n",
        "lossMatrix = []\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    lossMatrix.append(total_loss / steps_per_epoch)\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.2451\n",
            "Epoch 2 Batch 0 Loss 2.1686\n",
            "Epoch 2 Loss 2.1079\n",
            "Time taken for 1 epoch 20.16243553161621 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9826\n",
            "Epoch 4 Batch 0 Loss 1.8511\n",
            "Epoch 4 Loss 1.7214\n",
            "Time taken for 1 epoch 20.92720651626587 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5548\n",
            "Epoch 6 Batch 0 Loss 1.3135\n",
            "Epoch 6 Loss 1.3241\n",
            "Time taken for 1 epoch 17.756619691848755 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2623\n",
            "Epoch 8 Batch 0 Loss 1.0449\n",
            "Epoch 8 Loss 1.0087\n",
            "Time taken for 1 epoch 19.9654221534729 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8683\n",
            "Epoch 10 Batch 0 Loss 0.7822\n",
            "Epoch 10 Loss 0.7303\n",
            "Time taken for 1 epoch 17.951283931732178 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6162\n",
            "Epoch 12 Batch 0 Loss 0.5159\n",
            "Epoch 12 Loss 0.5253\n",
            "Time taken for 1 epoch 17.152433156967163 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4394\n",
            "Epoch 14 Batch 0 Loss 0.3263\n",
            "Epoch 14 Loss 0.3647\n",
            "Time taken for 1 epoch 19.278913497924805 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2828\n",
            "Epoch 16 Batch 0 Loss 0.2656\n",
            "Epoch 16 Loss 0.2722\n",
            "Time taken for 1 epoch 15.174743175506592 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2103\n",
            "Epoch 18 Batch 0 Loss 0.1720\n",
            "Epoch 18 Loss 0.2055\n",
            "Time taken for 1 epoch 17.08702039718628 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1883\n",
            "Epoch 20 Batch 0 Loss 0.1255\n",
            "Epoch 20 Loss 0.1779\n",
            "Time taken for 1 epoch 15.236437320709229 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1553\n",
            "Epoch 22 Batch 0 Loss 0.1376\n",
            "Epoch 22 Loss 0.1501\n",
            "Time taken for 1 epoch 15.50828218460083 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1279\n",
            "Epoch 24 Batch 0 Loss 0.1173\n",
            "Epoch 24 Loss 0.1369\n",
            "Time taken for 1 epoch 15.85084342956543 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1399\n",
            "Epoch 26 Batch 0 Loss 0.1070\n",
            "Epoch 26 Loss 0.1311\n",
            "Time taken for 1 epoch 15.642707347869873 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1278\n",
            "Epoch 28 Batch 0 Loss 0.0951\n",
            "Epoch 28 Loss 0.1241\n",
            "Time taken for 1 epoch 20.737025260925293 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0842\n",
            "Epoch 30 Batch 0 Loss 0.1297\n",
            "Epoch 30 Loss 0.1249\n",
            "Time taken for 1 epoch 15.578750371932983 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0984\n",
            "Epoch 32 Batch 0 Loss 0.0902\n",
            "Epoch 32 Loss 0.1176\n",
            "Time taken for 1 epoch 16.378813982009888 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.1070\n",
            "Epoch 34 Batch 0 Loss 0.0919\n",
            "Epoch 34 Loss 0.1174\n",
            "Time taken for 1 epoch 15.789714813232422 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0963\n",
            "Epoch 36 Batch 0 Loss 0.1085\n",
            "Epoch 36 Loss 0.1138\n",
            "Time taken for 1 epoch 19.2011456489563 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0737\n",
            "Epoch 38 Batch 0 Loss 0.0970\n",
            "Epoch 38 Loss 0.1121\n",
            "Time taken for 1 epoch 24.364898681640625 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0907\n",
            "Epoch 40 Batch 0 Loss 0.0939\n",
            "Epoch 40 Loss 0.1101\n",
            "Time taken for 1 epoch 16.118608236312866 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0900\n",
            "Epoch 42 Batch 0 Loss 0.1036\n",
            "Epoch 42 Loss 0.1116\n",
            "Time taken for 1 epoch 15.940951824188232 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.1136\n",
            "Epoch 44 Batch 0 Loss 0.0978\n",
            "Epoch 44 Loss 0.1100\n",
            "Time taken for 1 epoch 18.350285291671753 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.1002\n",
            "Epoch 46 Batch 0 Loss 0.0955\n",
            "Epoch 46 Loss 0.1048\n",
            "Time taken for 1 epoch 25.07692861557007 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0881\n",
            "Epoch 48 Batch 0 Loss 0.0818\n",
            "Epoch 48 Loss 0.1051\n",
            "Time taken for 1 epoch 22.863643407821655 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0818\n",
            "Epoch 50 Batch 0 Loss 0.0701\n",
            "Epoch 50 Loss 0.1035\n",
            "Time taken for 1 epoch 23.40096139907837 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0922\n",
            "Epoch 52 Batch 0 Loss 0.0929\n",
            "Epoch 52 Loss 0.1034\n",
            "Time taken for 1 epoch 20.559565544128418 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0935\n",
            "Epoch 54 Batch 0 Loss 0.0691\n",
            "Epoch 54 Loss 0.1050\n",
            "Time taken for 1 epoch 23.700353860855103 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0784\n",
            "Epoch 56 Batch 0 Loss 0.0798\n",
            "Epoch 56 Loss 0.1052\n",
            "Time taken for 1 epoch 17.077311515808105 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0953\n",
            "Epoch 58 Batch 0 Loss 0.0923\n",
            "Epoch 58 Loss 0.1011\n",
            "Time taken for 1 epoch 22.354254961013794 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0811\n",
            "Epoch 60 Batch 0 Loss 0.0761\n",
            "Epoch 60 Loss 0.1022\n",
            "Time taken for 1 epoch 16.59955906867981 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0976\n",
            "Epoch 62 Batch 0 Loss 0.0734\n",
            "Epoch 62 Loss 0.1004\n",
            "Time taken for 1 epoch 25.092862844467163 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0735\n",
            "Epoch 64 Batch 0 Loss 0.0852\n",
            "Epoch 64 Loss 0.1030\n",
            "Time taken for 1 epoch 21.04733443260193 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0950\n",
            "Epoch 66 Batch 0 Loss 0.1213\n",
            "Epoch 66 Loss 0.0996\n",
            "Time taken for 1 epoch 25.252251625061035 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1023\n",
            "Epoch 68 Batch 0 Loss 0.0764\n",
            "Epoch 68 Loss 0.1018\n",
            "Time taken for 1 epoch 24.89308452606201 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0773\n",
            "Epoch 70 Batch 0 Loss 0.0863\n",
            "Epoch 70 Loss 0.0984\n",
            "Time taken for 1 epoch 22.704904079437256 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0742\n",
            "Epoch 72 Batch 0 Loss 0.0963\n",
            "Epoch 72 Loss 0.0985\n",
            "Time taken for 1 epoch 25.630220651626587 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0765\n",
            "Epoch 74 Batch 0 Loss 0.0643\n",
            "Epoch 74 Loss 0.0975\n",
            "Time taken for 1 epoch 16.782602548599243 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1026\n",
            "Epoch 76 Batch 0 Loss 0.0759\n",
            "Epoch 76 Loss 0.0972\n",
            "Time taken for 1 epoch 16.635939598083496 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0985\n",
            "Epoch 78 Batch 0 Loss 0.0771\n",
            "Epoch 78 Loss 0.0985\n",
            "Time taken for 1 epoch 25.453782081604004 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0839\n",
            "Epoch 80 Batch 0 Loss 0.0741\n",
            "Epoch 80 Loss 0.0992\n",
            "Time taken for 1 epoch 17.52106237411499 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0890\n",
            "Epoch 82 Batch 0 Loss 0.0938\n",
            "Epoch 82 Loss 0.0975\n",
            "Time taken for 1 epoch 19.507304668426514 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0801\n",
            "Epoch 84 Batch 0 Loss 0.0832\n",
            "Epoch 84 Loss 0.0975\n",
            "Time taken for 1 epoch 17.08099865913391 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0865\n",
            "Epoch 86 Batch 0 Loss 0.0855\n",
            "Epoch 86 Loss 0.0963\n",
            "Time taken for 1 epoch 25.368056774139404 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0835\n",
            "Epoch 88 Batch 0 Loss 0.0913\n",
            "Epoch 88 Loss 0.0987\n",
            "Time taken for 1 epoch 17.274743795394897 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0897\n",
            "Epoch 90 Batch 0 Loss 0.0763\n",
            "Epoch 90 Loss 0.1004\n",
            "Time taken for 1 epoch 20.178911447525024 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0826\n",
            "Epoch 92 Batch 0 Loss 0.0869\n",
            "Epoch 92 Loss 0.0970\n",
            "Time taken for 1 epoch 17.98293685913086 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0839\n",
            "Epoch 94 Batch 0 Loss 0.0785\n",
            "Epoch 94 Loss 0.0969\n",
            "Time taken for 1 epoch 19.184688568115234 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0569\n",
            "Epoch 96 Batch 0 Loss 0.0772\n",
            "Epoch 96 Loss 0.0955\n",
            "Time taken for 1 epoch 17.989619493484497 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0715\n",
            "Epoch 98 Batch 0 Loss 0.0784\n",
            "Epoch 98 Loss 0.0920\n",
            "Time taken for 1 epoch 18.42931318283081 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0972\n",
            "Epoch 100 Batch 0 Loss 0.0963\n",
            "Epoch 100 Loss 0.0962\n",
            "Time taken for 1 epoch 16.35215926170349 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PG08nS0Y2Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAacXAiY2Lh",
        "colab_type": "code",
        "colab": {},
        "outputId": "32f8c54c-4772-4d85-a6ef-fb004ad79f8d"
      },
      "source": [
        "# load checklist object from saving\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9dd4453b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHp7g0UZY2Lj",
        "colab_type": "code",
        "colab": {},
        "outputId": "949ca88d-0323-4696-eea0-44b9af3dc95f"
      },
      "source": [
        "translate(\"let me leave !\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> let me leave ! <end>\n",
            "Predicted translation: আমাকে যেতে দাও ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}