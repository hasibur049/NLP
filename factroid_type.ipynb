{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "factroid_type.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Er1s22qW7VjxjInicXV4bGeQz7agLGuM",
      "authorship_tag": "ABX9TyMwu7uby/pq+8kZ1Hc2CUwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasiburrahman1/NLP/blob/master/factroid_type.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gInZeo8EQNqo"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as et \n",
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, SimpleRNN, GRU, LSTM, Bidirectional, Dropout, Input, Conv2D, MaxPool2D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input, GlobalMaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Alr0O10FzzG"
      },
      "source": [
        "####Convert XML into DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io1zBDGLnpbI"
      },
      "source": [
        "xtree = et.parse(\"/content/drive/My Drive/Colab Notebooks/MedQuAD_3_GHR_QA/joined.xml\")\n",
        "xroot = xtree.getroot()\n",
        "lst = xroot.findall('QAPairs/QAPair')\n",
        "data1 = []\n",
        "data2 = []\n",
        "data3 = []\n",
        "print(len(lst))\n",
        "#print(lst)\n",
        "print(\"\\n\")\n",
        "\n",
        "for item in lst:\n",
        "    for x in item:\n",
        "         if x.get('qtype') != None:\n",
        "            qtype = x.get('qtype')\n",
        "            data1.append(qtype)\n",
        "    Question = item.find('Question').text\n",
        "    data2.append(Question)\n",
        "    Answer = item.find('Answer').text\n",
        "    data3.append(Answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf-MgAmdBwF0"
      },
      "source": [
        "df = pd.DataFrame(list(zip(data1, data2, data3)), \n",
        "               columns =['qtype', 'Question', 'Answer']) \n",
        "\n",
        "df.drop(df.columns[[2]], axis = 1, inplace = True)\n",
        "print(len(df)) \n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp-qBcwpRJMk"
      },
      "source": [
        "###Basic data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQJOvjYA-dGG"
      },
      "source": [
        "df.to_csv('data.csv')\n",
        "!cp data.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKqebaX7PIOU"
      },
      "source": [
        "df[\"qtype\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OVPsJJpRFLo"
      },
      "source": [
        "## categorical to numerical\n",
        "encode = {\n",
        "    \"genetic changes\" : 0,\n",
        "    \"inheritance\"  : 1,       \n",
        "    \"frequency\" : 2,    \n",
        "    \"information\" : 3,     \n",
        "    \"treatment\"  : 4,    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf1l-EhVRwN7"
      },
      "source": [
        "df[\"qtype\"] = df[\"qtype\"].apply(lambda x: encode[x])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayj_CaUWSSy3"
      },
      "source": [
        "###Data cleaning for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUk6j7tHVC8o"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA3Ad6UOQulP"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    \n",
        "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    text = pattern.sub('', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
        "    \n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)        \n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text) \n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"did't\", \"did not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\"have't\", \"have not\", text)\n",
        "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVmv2EoiQ58F"
      },
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def CleanTokenize(df):\n",
        "    ques_lines = list()\n",
        "    lines = df[\"Question\"].values.tolist()\n",
        "\n",
        "    for line in lines:\n",
        "        line = clean_text(line)\n",
        "        # tokenize the text\n",
        "        tokens = word_tokenize(line)\n",
        "        # remove puntuations\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        # remove non alphabetic characters\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        \n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        # remove stop words\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        \n",
        "        ques_lines.append(words)\n",
        "        \n",
        "    return ques_lines\n",
        "\n",
        "ques_lines = CleanTokenize(df)\n",
        "print(\"total line are \", len(ques_lines)) \n",
        "ques_lines[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXSvF7WHe2jA"
      },
      "source": [
        "len(max(ques_lines, key=len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tz6-UrHWE4I"
      },
      "source": [
        "###Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45CY-fxhV4fD"
      },
      "source": [
        "validation_split = 0.2\n",
        "max_length = 15\n",
        "\n",
        "\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(ques_lines)\n",
        "sequences = tokenizer_obj.texts_to_sequences(ques_lines)\n",
        "\n",
        "word_index = tokenizer_obj.word_index\n",
        "print(\"unique tokens - \",len(word_index))\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "print('vocab size -', vocab_size)\n",
        "\n",
        "ques_lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "qtype =  df['qtype'].values\n",
        "\n",
        "indices = np.arange(ques_lines_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "ques_lines_pad = ques_lines_pad[indices]\n",
        "qtype = qtype[indices]\n",
        "\n",
        "n_values = np.max(qtype) + 1\n",
        "\n",
        "Y = np.eye(n_values)[qtype]\n",
        "\n",
        "\n",
        "num_validation_samples = int(validation_split * ques_lines_pad.shape[0])\n",
        "\n",
        "X_train_pad = ques_lines_pad[:-num_validation_samples]\n",
        "y_train = Y[:-num_validation_samples]\n",
        "X_test_pad = ques_lines_pad[-num_validation_samples:]\n",
        "y_test = Y[-num_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJRaXreKW6PB"
      },
      "source": [
        "print('Shape of X_train_pad:', X_train_pad.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n",
        "\n",
        "print('Shape of X_test_pad:', X_test_pad.shape)\n",
        "print('Shape of y_test:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liJHecjkXVxo"
      },
      "source": [
        "###Custom word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcnFTO4BXNV1"
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec(sentences=ques_lines, size=100, window=5, workers=4, min_count=1, sg=1) #sg= 1:skip-gram 0:cbow\n",
        "vocab_words = list(model.wv.vocab)   \n",
        "\n",
        "print(len(vocab_words))\n",
        "print(vocab_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maJ9urB6XtmV"
      },
      "source": [
        "filename = \"questype_word2vec_full.txt\"\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFywa32hX_Zn"
      },
      "source": [
        "embedding_index = {}\n",
        "f = open(os.path.join('', '/content/questype_word2vec_full.txt'),  encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coeff = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_index[word] = coeff\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlDSUeGHYPE8"
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X_NfOpczKzM"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiXF4i1QYTcn"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldYOKB7cYa8U"
      },
      "source": [
        "##Create LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Xhx2L6_Qwr"
      },
      "source": [
        "# https://rdrr.io/cran/tfaddons/man/metrics_f1score.html\n",
        "# https://github.com/tensorflow/addons/issues/825\n",
        "model1 = Sequential()\n",
        "model1.add(embedding_layer)\n",
        "# Recurrent layer\n",
        "model1.add(LSTM(100, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "# Fully connected layer\n",
        "model1.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.FBetaScore(num_classes=5, average=\"macro\", threshold=0.5 )])\n",
        "\n",
        "print(model1.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAqhSMdMRn0L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSmX9abecyI2"
      },
      "source": [
        "\n",
        "###LSTM Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeR_z2uzYvIj"
      },
      "source": [
        "history = model1.fit(X_train_pad, y_train, epochs=10,batch_size=32, validation_data=(X_test_pad, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b_Hca6Z7_Gb"
      },
      "source": [
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model train vs validation loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYW_Hype1VRU"
      },
      "source": [
        "####Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKkIKsmsVVRZ"
      },
      "source": [
        "\n",
        "model2 =Sequential()\n",
        "model2.add(embedding_layer)\n",
        "\n",
        "model2.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.25)))\n",
        "model2.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.FBetaScore(num_classes=5, average=\"macro\", threshold=0.5)])\n",
        "\n",
        "print(model2.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5BZB2UfuWM6"
      },
      "source": [
        "history = model2.fit(X_train_pad, y_train, epochs=10,batch_size=32, validation_data=(X_test_pad, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUiDfIhCVWqm"
      },
      "source": [
        "###GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMl72nGL1enC"
      },
      "source": [
        "\n",
        "#https://www.programcreek.com/python/example/97114/keras.layers.recurrent.GRU\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(embedding_layer)\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(GRU(100, return_sequences=False))\n",
        "# Add dropout if overfitting\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(5))\n",
        "model3.add(Activation('softmax'))\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.FBetaScore(num_classes=5, average=\"macro\", threshold= .5)])\n",
        "model3.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTZY_L_E5ax9"
      },
      "source": [
        "history = model3.fit(X_train_pad, y_train, epochs=10,batch_size=60, validation_data=(X_test_pad, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3lKlYxJyb1N"
      },
      "source": [
        "###Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ9dub5MyaM9"
      },
      "source": [
        "\n",
        "#https://medium.com/@hemantranvir/spam-detection-using-rnn-simplernn-lstm-with-step-by-step-explanation-530367608071\n",
        "model4 = Sequential()\n",
        "model4.add(embedding_layer)\n",
        "#model.add(SimpleRNN(units=embedding_mat_columns))\n",
        "model4.add(SimpleRNN(units=100, dropout=0.2, recurrent_dropout=0.25))\n",
        "model4.add(Dense(5, activation='softmax'))\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.FBetaScore(num_classes=5, average=\"macro\", threshold= 0.5 )])\n",
        "model4.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAtKhr6wziaS"
      },
      "source": [
        "history = model4.fit(X_train_pad, y_train, epochs=10,batch_size=60, validation_data=(X_test_pad, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BavxI1y9jIc"
      },
      "source": [
        "####CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i90UNmRAiGb"
      },
      "source": [
        "#https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py----------\n",
        "#https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
        "# set parameters:\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 100\n",
        "\n",
        "model5 = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "\n",
        "#model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "model5.add(embedding_layer)\n",
        "model5.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model5.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model5.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model5.add(Dense(hidden_dims))\n",
        "model5.add(Dropout(0.2))\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model5.add(Dense(5))\n",
        "model5.add(Activation('softmax'))\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.FBetaScore(num_classes=5, average=\"macro\", threshold= 0.5 )])\n",
        "\n",
        "model5.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA937nDv-m5t"
      },
      "source": [
        "history = model5.fit(X_train_pad, y_train, epochs=10,batch_size=60, validation_data=(X_test_pad, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxk7iXe8o6N"
      },
      "source": [
        "\n",
        "###Can this model detect qtype from LSTM model?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7lm3azQ8S_2"
      },
      "source": [
        "from pandas import *\n",
        "import operator\n",
        "\n",
        "def predict_(s):\n",
        "    x_final = pd.DataFrame({\"Question\":[s]})\n",
        "    #print(\"x final\",x_final)\n",
        "    test_lines = CleanTokenize(x_final)\n",
        "    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
        "    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "    pred = model1.predict(test_review_pad)\n",
        "    #print(f'NumPy Array:\\n{[pred]}')\n",
        "    pred = pred.ravel()\n",
        "    listOfInt  = pred.tolist()\n",
        "    #print(listOfInt)\n",
        "    listOfStr = [\"genetic changes\", \"inheritance\", \"frequency\" , \"information\" , \"treatment\"]\n",
        "    # Create a zip object from two lists\n",
        "    zipbObj = zip(listOfStr, listOfInt)\n",
        "    # Create a dictionary from zip object\n",
        "    dictOfresult = dict(zipbObj)\n",
        "    #print(s, end =\" \")\n",
        "    #print(\"predict type:\" max(dictOfresult.items(), key=operator.itemgetter(1))[0])\n",
        "    return max(dictOfresult.items(), key=operator.itemgetter(1))[0]\n",
        "\n",
        "def qtype_encode(actual_qtype):\n",
        "    if actual_qtype == 0:\n",
        "        return 'genetic changes'      \n",
        "    elif actual_qtype == 1: \n",
        "        return 'inheritance'\n",
        "    elif  actual_qtype == 2: \n",
        "        return 'frequency'\n",
        "    elif  actual_qtype == 3: \n",
        "        return 'information'\n",
        "    elif  actual_qtype == 4: \n",
        "        return 'treatment'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWgTgO_TRqOb"
      },
      "source": [
        "i = 4000\n",
        "for k in range(i,i+50):\n",
        "    ques = df['Question'].iloc[k]  \n",
        "    qtype = df['qtype'].iloc[k]\n",
        "    predict_qtype = predict_(ques)\n",
        "    actual_qtype = qtype_encode(qtype)\n",
        "    print(\"Question : \",ques ,\"\\nActual type :\",actual_qtype,\"\\nPredict type : \",predict_qtype,\"\\n\\n\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALDXxXV4-3a0"
      },
      "source": [
        "predict_(\"What is (are) Coronavirous syndrome ?\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ4AIl8FDvyF"
      },
      "source": [
        "predict_(\"How many people are affected by Coronavirous syndrome ?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpDmeEsdDzAC"
      },
      "source": [
        "predict_(\"Is Coronavirous inherited ?\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}