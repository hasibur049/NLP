{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of MachineTranslationEn-Bn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasiburrahman1/NLP/blob/master/Copy_of_MachineTranslationEn_Bn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnhpcQJY5XC",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation English to Bangla with RNN\n",
        "\n",
        "Machine Translation using Recurrent Neural Network\n",
        "> License https://www.apache.org/licenses/LICENSE-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKM0J7SY5XE",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "- Install Anaconda `Jupyter` Notebook.\n",
        "- Install Python3.\n",
        "- With Anaconda Prompt install `Tensorflow Keras` NLP Tools. For Faster Processing need GPU processing. \n",
        "    - Keras\n",
        "```batch\n",
        "   conda install -c conda-forge tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2zIvMc6yhm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e30a5682-da59-4d17-80b6-beb8f99f937e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWVprBmpY5XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkKjrUOHY5XJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "74c21561-4fb3-47ea-9b59-41f680095e6a"
      },
      "source": [
        "# check up GPU/CPU processing power\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6030797894728383451\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8625444758503793106\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcdT2VfOY5XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset path\n",
        "path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/datasetCleaned.txt\"\n",
        "#path_to_file = \"/content/drive/My Drive/Colab Notebooks/dataset/test_Text_Document.txt\""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrJEUQPY5XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "#     remove punctuations from lines\n",
        "    w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d3rfmJcY5XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabularyCounter(dataFileEn, dataFileBn):\n",
        "    # setup Vocabulary \n",
        "    english_words = collections.Counter([word for sentence in dataFileEn for word in sentence.split()])\n",
        "    bangla_words = collections.Counter([word for sentence in dataFileBn for word in sentence.split()])\n",
        "    \n",
        "    print('Total English words: {}'.format(len([word for sentence in dataFileEn for word in sentence.split()])))\n",
        "    print('Unique English words: {}'.format(len(english_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*english_words.most_common(10)))[0]) + '\"\\n')\n",
        "\n",
        "    print('Total Bangla words: {}'.format(len([word for sentence in dataFileBn for word in sentence.split()])))\n",
        "    print('Unique Bangla words: {}'.format(len(bangla_words)))\n",
        "    print('Most Common Words: \"' + '\" \"'.join(list(zip(*bangla_words.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpulyhIUY5XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, BANGLA]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    allEnLines = []\n",
        "    allBnLines = []\n",
        "    for l in lines[:num_examples]:\n",
        "        w = l.split('\\t')\n",
        "        allEnLines.append(w[0])\n",
        "        allBnLines.append(w[1])\n",
        "    vocabularyCounter(allEnLines, allBnLines) \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "#     rearrange dataset src and target\n",
        "    for indd in range(len(word_pairs)):\n",
        "        temp = word_pairs[indd][0]\n",
        "        word_pairs[indd][0] = word_pairs[indd][1]\n",
        "        word_pairs[indd][1] = temp\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T4YavuYY5Xa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "5aefecea-0d84-4489-d700-888f44713327"
      },
      "source": [
        "# preprocessing function declear and vocabulary checking\n",
        "bn, en = create_dataset(path_to_file, None)\n",
        "print(\"\\n\\n\", en[4])\n",
        "print(bn[4])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 19606\n",
            "Unique English words: 2839\n",
            "Most Common Words: \"I\" \"Tom\" \"to\" \"you\" \"is\" \"the\" \"a\" \"I'm\" \"your\" \"He\"\n",
            "\n",
            "Total Bangla words: 19000\n",
            "Unique Bangla words: 3527\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"কি\" \"না।\" \"আপনি\" \"এটা\" \"তুমি\" \"কথা\" \"আপনার\"\n",
            "\n",
            "\n",
            " <start> Run ! <end>\n",
            "<start> পালান ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-MgBVD5Y5Xe",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sew4h85PY5Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function declearation for \n",
        "#     1. length of sentence\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#     2. tokenizer\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "#     3. load dataset\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang  = create_dataset(path, num_examples)\n",
        "    \n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ki0zuXY5Xh",
        "colab_type": "text"
      },
      "source": [
        "## Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvjgS2pyY5Xh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "b7d8a3b5-86d0-4b63-d1b4-461c5e53a706"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(\"Size:- Train_X-\", len(input_tensor_train), \"Train_Y-\", len(target_tensor_train), \"Test_X-\", len(input_tensor_val), \"Test_Y-\", len(target_tensor_val))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total English words: 2425\n",
            "Unique English words: 555\n",
            "Most Common Words: \"I\" \"Tom\" \"Tom.\" \"I'm\" \"is\" \"He\" \"Who\" \"You\" \"me.\" \"it.\"\n",
            "\n",
            "Total Bangla words: 2645\n",
            "Unique Bangla words: 770\n",
            "Most Common Words: \"আমি\" \"টম\" \"আমার\" \"টমকে\" \"আমরা\" \"আমাকে\" \"না।\" \"এটা\" \"কি\" \"করুন।\"\n",
            "Size:- Train_X- 800 Train_Y- 800 Test_X- 200 Test_Y- 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjgMFIzaY5Xj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a26f22c0-8793-47f6-9783-456749414b32"
      },
      "source": [
        "# showing example of tokenization\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "18 ----> are\n",
            "30 ----> we\n",
            "167 ----> ready\n",
            "6 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "12 ----> আমরা\n",
            "15 ----> কি\n",
            "198 ----> তৈরি\n",
            "5 ----> ?\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M04kXdwjY5Xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a78407d-2296-445c-bbf4-92e172015951"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print(\"Input-output shape batch\", example_input_batch.shape, example_target_batch.shape)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input-output shape batch (64, 7) (64, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ChByDVVY5Xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "001a23fe-38f1-4a61-bdf8-b9d43ea51ffb"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, activation='linear',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 7, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb0TGNahY5Xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e79a25b-f6c8-4e5e-d37b-779b9d84c973"
      },
      "source": [
        "# Attention and context_vector\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.sigmoid(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    attention_weights = tf.nn.sigmoid(score)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh3amf2uY5Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a52032ab-9575-4582-ab25-1b00dda7433d"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, activation='tanh',return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 673)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4pWS6iMY5Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk7BlC05Y5Xv",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoint saving object "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH44B_ifY5Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoint for saving the object\n",
        "checkpoint_dir = './training_checkpoints_saved'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpointSaved\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnx9VvCAY5Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIYhouznY5X0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ead47f6-b53f-4d83-f6ea-9d3408d9f598"
      },
      "source": [
        "# start learn processing\n",
        "EPOCHS = 100\n",
        "lossMatrix = []\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    lossMatrix.append(total_loss / steps_per_epoch)\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.2562\n",
            "Epoch 2 Batch 0 Loss 2.1837\n",
            "Epoch 2 Loss 2.0693\n",
            "Time taken for 1 epoch 21.755837440490723 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9614\n",
            "Epoch 4 Batch 0 Loss 1.6782\n",
            "Epoch 4 Loss 1.6247\n",
            "Time taken for 1 epoch 21.413087844848633 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4487\n",
            "Epoch 6 Batch 0 Loss 1.2466\n",
            "Epoch 6 Loss 1.2008\n",
            "Time taken for 1 epoch 21.480656385421753 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1256\n",
            "Epoch 8 Batch 0 Loss 0.8874\n",
            "Epoch 8 Loss 0.8805\n",
            "Time taken for 1 epoch 21.6071834564209 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.7132\n",
            "Epoch 10 Batch 0 Loss 0.5763\n",
            "Epoch 10 Loss 0.6240\n",
            "Time taken for 1 epoch 21.09920620918274 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5213\n",
            "Epoch 12 Batch 0 Loss 0.4032\n",
            "Epoch 12 Loss 0.4346\n",
            "Time taken for 1 epoch 21.175647497177124 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3696\n",
            "Epoch 14 Batch 0 Loss 0.2610\n",
            "Epoch 14 Loss 0.3118\n",
            "Time taken for 1 epoch 21.63126277923584 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2590\n",
            "Epoch 16 Batch 0 Loss 0.1811\n",
            "Epoch 16 Loss 0.2271\n",
            "Time taken for 1 epoch 21.382924795150757 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1946\n",
            "Epoch 18 Batch 0 Loss 0.1585\n",
            "Epoch 18 Loss 0.1830\n",
            "Time taken for 1 epoch 21.243727207183838 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1258\n",
            "Epoch 20 Batch 0 Loss 0.1213\n",
            "Epoch 20 Loss 0.1509\n",
            "Time taken for 1 epoch 21.834089040756226 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0996\n",
            "Epoch 22 Batch 0 Loss 0.1111\n",
            "Epoch 22 Loss 0.1455\n",
            "Time taken for 1 epoch 21.287967920303345 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1182\n",
            "Epoch 24 Batch 0 Loss 0.1251\n",
            "Epoch 24 Loss 0.1320\n",
            "Time taken for 1 epoch 20.91904854774475 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1081\n",
            "Epoch 26 Batch 0 Loss 0.1022\n",
            "Epoch 26 Loss 0.1292\n",
            "Time taken for 1 epoch 21.02054190635681 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1131\n",
            "Epoch 28 Batch 0 Loss 0.0988\n",
            "Epoch 28 Loss 0.1246\n",
            "Time taken for 1 epoch 20.59879493713379 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.1279\n",
            "Epoch 30 Batch 0 Loss 0.1076\n",
            "Epoch 30 Loss 0.1221\n",
            "Time taken for 1 epoch 20.645390033721924 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.1088\n",
            "Epoch 32 Batch 0 Loss 0.0972\n",
            "Epoch 32 Loss 0.1175\n",
            "Time taken for 1 epoch 20.932289361953735 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0825\n",
            "Epoch 34 Batch 0 Loss 0.0949\n",
            "Epoch 34 Loss 0.1129\n",
            "Time taken for 1 epoch 21.2285418510437 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.1011\n",
            "Epoch 36 Batch 0 Loss 0.0943\n",
            "Epoch 36 Loss 0.1110\n",
            "Time taken for 1 epoch 20.796719312667847 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.1130\n",
            "Epoch 38 Batch 0 Loss 0.0973\n",
            "Epoch 38 Loss 0.1091\n",
            "Time taken for 1 epoch 22.22139620780945 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0950\n",
            "Epoch 40 Batch 0 Loss 0.1005\n",
            "Epoch 40 Loss 0.1095\n",
            "Time taken for 1 epoch 20.83688259124756 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0871\n",
            "Epoch 42 Batch 0 Loss 0.0779\n",
            "Epoch 42 Loss 0.1077\n",
            "Time taken for 1 epoch 20.327964067459106 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0924\n",
            "Epoch 44 Batch 0 Loss 0.0741\n",
            "Epoch 44 Loss 0.1064\n",
            "Time taken for 1 epoch 20.261651515960693 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0732\n",
            "Epoch 46 Batch 0 Loss 0.0837\n",
            "Epoch 46 Loss 0.1057\n",
            "Time taken for 1 epoch 20.554266452789307 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0595\n",
            "Epoch 48 Batch 0 Loss 0.0871\n",
            "Epoch 48 Loss 0.1048\n",
            "Time taken for 1 epoch 20.718178749084473 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0984\n",
            "Epoch 50 Batch 0 Loss 0.0652\n",
            "Epoch 50 Loss 0.1029\n",
            "Time taken for 1 epoch 20.61936116218567 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0847\n",
            "Epoch 52 Batch 0 Loss 0.0764\n",
            "Epoch 52 Loss 0.1017\n",
            "Time taken for 1 epoch 20.826679944992065 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.1003\n",
            "Epoch 54 Batch 0 Loss 0.0882\n",
            "Epoch 54 Loss 0.0989\n",
            "Time taken for 1 epoch 20.586206674575806 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0792\n",
            "Epoch 56 Batch 0 Loss 0.0952\n",
            "Epoch 56 Loss 0.1034\n",
            "Time taken for 1 epoch 20.546319484710693 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0884\n",
            "Epoch 58 Batch 0 Loss 0.0746\n",
            "Epoch 58 Loss 0.1012\n",
            "Time taken for 1 epoch 20.80865478515625 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0734\n",
            "Epoch 60 Batch 0 Loss 0.0783\n",
            "Epoch 60 Loss 0.1008\n",
            "Time taken for 1 epoch 21.249887228012085 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0717\n",
            "Epoch 62 Batch 0 Loss 0.0860\n",
            "Epoch 62 Loss 0.1012\n",
            "Time taken for 1 epoch 21.652466773986816 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0871\n",
            "Epoch 64 Batch 0 Loss 0.0729\n",
            "Epoch 64 Loss 0.0958\n",
            "Time taken for 1 epoch 21.056659936904907 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0768\n",
            "Epoch 66 Batch 0 Loss 0.0910\n",
            "Epoch 66 Loss 0.0979\n",
            "Time taken for 1 epoch 20.944420099258423 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0750\n",
            "Epoch 68 Batch 0 Loss 0.0757\n",
            "Epoch 68 Loss 0.0958\n",
            "Time taken for 1 epoch 21.07130527496338 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0987\n",
            "Epoch 70 Batch 0 Loss 0.0739\n",
            "Epoch 70 Loss 0.0974\n",
            "Time taken for 1 epoch 22.26449751853943 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0894\n",
            "Epoch 72 Batch 0 Loss 0.0805\n",
            "Epoch 72 Loss 0.0958\n",
            "Time taken for 1 epoch 20.96216106414795 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0906\n",
            "Epoch 74 Batch 0 Loss 0.0764\n",
            "Epoch 74 Loss 0.0979\n",
            "Time taken for 1 epoch 20.36210608482361 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0832\n",
            "Epoch 76 Batch 0 Loss 0.1059\n",
            "Epoch 76 Loss 0.1006\n",
            "Time taken for 1 epoch 20.677515268325806 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0626\n",
            "Epoch 78 Batch 0 Loss 0.0691\n",
            "Epoch 78 Loss 0.0977\n",
            "Time taken for 1 epoch 21.01936364173889 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0834\n",
            "Epoch 80 Batch 0 Loss 0.0685\n",
            "Epoch 80 Loss 0.0987\n",
            "Time taken for 1 epoch 20.74841570854187 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0783\n",
            "Epoch 82 Batch 0 Loss 0.0758\n",
            "Epoch 82 Loss 0.0973\n",
            "Time taken for 1 epoch 20.474672555923462 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0996\n",
            "Epoch 84 Batch 0 Loss 0.0867\n",
            "Epoch 84 Loss 0.0945\n",
            "Time taken for 1 epoch 20.541909217834473 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0812\n",
            "Epoch 86 Batch 0 Loss 0.0506\n",
            "Epoch 86 Loss 0.0919\n",
            "Time taken for 1 epoch 20.62467312812805 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0658\n",
            "Epoch 88 Batch 0 Loss 0.0801\n",
            "Epoch 88 Loss 0.0916\n",
            "Time taken for 1 epoch 20.894939661026 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0570\n",
            "Epoch 90 Batch 0 Loss 0.0674\n",
            "Epoch 90 Loss 0.0937\n",
            "Time taken for 1 epoch 21.34113097190857 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0791\n",
            "Epoch 92 Batch 0 Loss 0.0881\n",
            "Epoch 92 Loss 0.0962\n",
            "Time taken for 1 epoch 21.070618629455566 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0894\n",
            "Epoch 94 Batch 0 Loss 0.0911\n",
            "Epoch 94 Loss 0.0898\n",
            "Time taken for 1 epoch 21.280333518981934 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0864\n",
            "Epoch 96 Batch 0 Loss 0.0923\n",
            "Epoch 96 Loss 0.0932\n",
            "Time taken for 1 epoch 21.475286722183228 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0808\n",
            "Epoch 98 Batch 0 Loss 0.0765\n",
            "Epoch 98 Loss 0.0922\n",
            "Time taken for 1 epoch 21.577060222625732 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0836\n",
            "Epoch 100 Batch 0 Loss 0.0803\n",
            "Epoch 100 Loss 0.0889\n",
            "Time taken for 1 epoch 21.365160942077637 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybmmiLecY5X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycg_V79-Y5X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c844c3f7-96ae-4f9a-902f-6db3e1799e71"
      },
      "source": [
        "# load checklist object from saving\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7febfd45d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOEHCtTBY5X7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "09319528-d559-471c-eaad-548f793cdf8a"
      },
      "source": [
        "translate(\"where you live !\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> where you live ! <end>\n",
            "Predicted translation: তুমি কোথায় ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNVFHJE8KMl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ebe5c4bd-52c7-46fa-9ad2-55c4c48bc64a"
      },
      "source": [
        "translate(\"you will get well\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> you will get well <end>\n",
            "Predicted translation: আমি যাবো । <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpyYHfHmKqUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "d0087f33-6a30-40fa-ec2a-61c794a83bf5"
      },
      "source": [
        "translate(\"get well soon!\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-f1750a936887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get well soon!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-b2b547e92726>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-b2b547e92726>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                            \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-b2b547e92726>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                            \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'soon'"
          ]
        }
      ]
    }
  ]
}